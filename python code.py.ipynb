{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ed2122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd import numpy as np\n",
    "#to visualize the dataset import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "sns.set()\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighbors\n",
    "Classifier\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall\n",
    "score\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "from keras import regularizers, Sequential\n",
    "Xmatplotlib inline\n",
    "sns.set(style='whitegrid\", palette='muted', font_scale=1.5)\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "RANDOM SEED 42\n",
    "LABELS (\"Normal\", \"Fraud\"]\n",
    "import warnings\n",
    "warnings.filterwarnings ignore) df=pd.read_csv('/content/drive/MyDrive/creditcard.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()\n",
    "#exploring the datatype of each column\n",
    "df.info()\n",
    "# data shpae\n",
    "df.shape\n",
    "#Lets see the column name\n",
    "df.columns\n",
    "count_classes = pd.value_counts (df['Class'], sort = True)\n",
    "count_classes.plot(kind= 'bar', rot=0)\n",
    "plt.title(\"Transaction class distribution\")\n",
    "plt.xticks (range(2))\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\");\n",
    "frauds df [df.Class == 1]\n",
    "normal df[df.Class == 0]\n",
    "\n",
    "frauds.shape\n",
    "normal.shape\n",
    "#Summary Statistics\n",
    "df.describe()\n",
    "x = raw_data.drop('Class', axis=1)\n",
    "y=raw_data['Class']\n",
    ")\n",
    "yra duta 'Class\"]\n",
    "strain val.x test,y train val.y_test-train test split(x,y, test size 0.15, random state, stratify y) train, val,\n",
    "ytrain, y val train test split(x train val, y_train_val, test size 0.15, random state-o, stratify y_train_val)\n",
    "attack data rau data[rau data[class]→1) normal datara data[ra data[ Class]]\n",
    "print(fluster of Source Attack Record print(\"Nunber of Source Normal Record (len(attack data)) with\n",
    "ratio (round((len(attack_data)/len(attack_data)/len (raw_data))*100,4))) (len(normal data)) with ratio\n",
    "(round((len(normal_data)/len(normal_data)/len(raw_data))*100,4)) 5')\n",
    "ass\"], sort = True)\n",
    "printf sumber of test attack Record protuber of test Normal Record (y_test.value_counts()[1]) with\n",
    "ratio (round(y_test.value_counts()[1]/len(y_test)*180,4)) X') (y_test.value_counts()[0]) with ratic\n",
    "(round(y_test.value_counts()[@]/len(y_test)*100,4)) X')\n",
    "San\")\n",
    "Print(arenumber of train Attack Record print Flumber of train Normal Record (y\n",
    "train.value_counts()[1]) with ratio (round(y_train.value_counts()[1]/len(y_train)*100,4)) %\") (y\n",
    "train.value_counts()[0]) with ratio (round(y_train.value counts()[8]/len(y_train)*100,4)) X')\n",
    "print(\"Number of Validate Attack Record protuber of validate Normal Record\n",
    "(y_val.value_counts() [1]) with ratio (round(y_val.value_counts() [1]/len(y_val)*100,4)) X')\n",
    "(y_val.value counts()[0]) with ratio (round(y_val.value_counts()[8]/len(y_val)*100,4)) X')\n",
    "train data-pd.concat(x_train,y train), axis -1)\n",
    "trale data\n",
    "\n",
    "from skleurm.neighbors Inport KleighborsClassifier\n",
    "CM model KheighborsClassifier(n_neighbors. 5) CM model fits train,y train)\n",
    "pred-model.predictix_test from sklearn.linear_model\n",
    "import LogisticRegression\n",
    "model -LogisticRegression(max_iter = 1000)\n",
    "model.fit(x_train,y_train)\n",
    "y_pred_proba KNN_model.predict_proba(x_test)\n",
    "y_pred_proba\n",
    "y_pred_edited-y_pred_proba[:,1]>0.2\n",
    "y_pred_edited-y_pred_edited.astype(int)\n",
    "y pred edited\n",
    "from sklearn import metrics\n",
    "class_report metrics.classification_report(y_test,y_pred)\n",
    "print(class_report)\n",
    "accuaracy metrics.accuracy_score(y_test, y_pred)\n",
    "recall metrics.recall_score(y_test, y_pred)\n",
    "precision metrics.precision_score(y_test,y_pred) f1_score metrics.f1_score(y_test,y_pred)\n",
    "print('Accuaracy, accuaracy.round(3)*100,'%')\n",
    "print('Recall recall.round(3)*100, '%')\n",
    "print('Precision, precision.round(3)*100, '%')\n",
    "print('F1_score, f1_score.round(3)*100, '%')\n",
    "import matplotlib.pyplot as plt\n",
    "fpr, tpr, tresholds metrics.roc_curve(y_test,y_pred_proba[:,1])\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0,1], [0,1], '--')\n",
    "AUC metrics.roc_auc_score(y_test,y_pred_proba[:,1])\n",
    "\n",
    "print AUC AUC.round(3) 100%\n",
    "✓ Normalization\n",
    "from sklearn.preprocessing inport No\n",
    "normalizer Normalizer (norm-'12\")\n",
    "print(normalizer.fit_transform(df))\n",
    "#distribution of legitimate transac\n",
    "df['Class\").value_counts()\n",
    "#separating the data for analysis\n",
    "legit df(df['Class\"]\n",
    "fraud dfdf['Class\"]\n",
    "11\n",
    "#statistical measures of the legit d\n",
    "legit.describe()\n",
    "legit.Amount.describe() statistical measures of the fraud da\n",
    "# fraud.describe()\n",
    "fraud.Amount.describe()\n",
    "compare the values for both transact df.groupby (Class\").mean\n",
    "legit sample legit.sample(n-492)\n",
    "Concatenating two DataFrames\n",
    "new_df pd.concat([legit_sample, fraud\n",
    "Print first 5 rows of the new dataset\n",
    "new_df.head()\n",
    "Getting the distribution of the class\n",
    "new df['Class\"].value_counts()\n",
    "Check Missing values\n",
    "\n",
    "df.isnull().sum().sort values(ascending\n",
    "new df.shape\n",
    "Splitting the data into Features & Tar new_df.drop(columns-Class, axis-1)\n",
    "New\n",
    "Splitting the data into Training data & Testing data\n",
    "x_train, x_test, y train, y test train_test_split(x, y, test sizeve.2, random state-42)\n",
    "Check whether the data is splitted in 80:20 ratio print(x.shape, X_train.shape, x_test.shape)\n",
    "Check whether the data is splitted in se:20 ratio print(X.shape, x_train.shape, x_test.shape)\n",
    "#call the Hodel\n",
    "model LogisticRegression(random_state=42)\n",
    "import pipeline\n",
    "from sklearn.pipeline import Pipeline Create a pipeline for each model\n",
    "pipeline Pipeline([\n",
    "('model', model)\n",
    "1)\n",
    "Perform cross-validation\n",
    "scores cross val_score(pipeline, x_train, y train, cv-5)\n",
    "Calculate mean accuracy\n",
    "mean accuracy scores.mean()\n",
    "Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y train) Make predictions on the test data\n",
    "y pred pipeline.predict(X_test)\n",
    "calculate accuracy score\n",
    "accuracy accuracy_score(y_test, y_pred)\n",
    "print(\"Hodel\", LogisticRegression())\n",
    "print(\"Cross-validation Accuracy:\", mean accuracy) print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "print('Recall score: recall score(y_test, y_pred)) print('Precision Score:', precision_score(y_test, y\n",
    "pred)>\n",
    "best model pipeline\n",
    "#save the best model\n",
    "import pickle\n",
    "pickle.dump(best_model, open('iris_model.dot', 'wb'))\n",
    "#visulalizing the confusion matrix\n",
    "LABELS [Normal', 'Fraud']\n",
    "from sklearn.metrics import confusion_matrix conf matrix confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize =(6, 6))\n",
    "sns.heatmap(conf_matrix, xticklabels LABELS, yticklabels LABELS, annot True, fat \"d\");\n",
    "plt.title(\"Confusion matrix\") plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.show\n",
    "Input Input (shape-10, nameInput')\n",
    "Encaster Layers encoder Dense (64, activation-relu\", name-encoder) (Input)\n",
    "encoder Dense(33, activations'relu, nas ancoder Densete, activatione relu, name-encoder) (encoder)\n",
    "Decoder Layers\n",
    "decader Dense (17, activation rela, name decoder) (encoder)\n",
    "decoder Densa of, activation relu, name decoder) (decoder) decoder Dunse(10, activation-lo, name\n",
    "decolor) (decoder)\n",
    "Define model\n",
    "modal Model(Inputs-Input, outputs-decoder)\n",
    "nodel.summary\n",
    "mode) compile(optimizera, lasse Input layer Input (shape(X. shape(1).3)\n",
    "\n",
    "encoding part\n",
    "encoded Dense(10), activation tanh, artivity, regularlsregilarters.11(-1)t layer) encodet Dense(se,\n",
    "activation ralu) (encoded)\n",
    "we decaling part\n",
    "decoded Dense (se, activation tanh) (encoder) decoded Dansa 100, activation tanh decoded\n",
    "## output lever\n",
    "output Layer Dense (Kshape(1), activation pels) (decoded) autoencoder Model(Input layer, output\n",
    "layer)\n",
    "data.drop(Class), ace) y data Class\" values\n",
    "autuencoder adulte, louss scalexicaler), fit transforms,values Tors(values) nere, frautscale(y),\n",
    "xscalely1] autoencoder.fit(x_norm[0:2000], x_norm[6:2000], batch size 512, epochs 30,\n",
    "shuffle True, validation split 8.20);\n",
    "hidden_representation Sequential()\n",
    "hidden representation.add(autoencoder.layers[0])\n",
    "hidden representation.add(autoencoder.layers[1])\n",
    "hidden_representation.add(autoencoder.layers[2])\n",
    "norm_hid_rep hidden_representation.predict(x_norm[:3000])\n",
    "fraud_hid_rep hidden_representation.predict(x_fraud) rep x np.append(norm_hid_rep,\n",
    "fraud_hid_rep, axis 0)\n",
    "y_nnp.zeros(norm_hid_rep.shape[0])\n",
    "y_fnp.ones(fraud_hid_rep.shape[0])\n",
    "rep_y np.append(y_n, y_f)\n",
    "train x, val x, train y, valy train_test_split(rep_x, rep_y, test size-0.25)\n",
    "clf LogisticRegression (solver-\"1bfgs\").fit(train x, train y)\n",
    "pred_y clf.predict(val_x)\n",
    "print(\"\")\n",
    "\n",
    "print (\"Classification Report: \")\n",
    "print (classification_report(val_y, pred_y))\n",
    "print (\"\")\n",
    "print Accuracy Score:\", accuracy_score(val_y, pred_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1ed518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd import numpy as np\n",
    "#to visualize the dataset import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "sns.set()\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighbors\n",
    "Classifier\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall\n",
    "score\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "from keras import regularizers, Sequential\n",
    "Xmatplotlib inline\n",
    "sns.set(style='whitegrid\", palette='muted', font_scale=1.5)\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "RANDOM SEED 42\n",
    "LABELS (\"Normal\", \"Fraud\"]\n",
    "import warnings\n",
    "warnings.filterwarnings ignore) df=pd.read_csv('/content/drive/MyDrive/creditcard.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()\n",
    "#exploring the datatype of each column\n",
    "df.info()\n",
    "# data shpae\n",
    "df.shape\n",
    "#Lets see the column name\n",
    "df.columns\n",
    "count_classes = pd.value_counts (df['Class'], sort = True)\n",
    "count_classes.plot(kind= 'bar', rot=0)\n",
    "plt.title(\"Transaction class distribution\")\n",
    "plt.xticks (range(2))\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\");\n",
    "frauds df [df.Class == 1]\n",
    "normal df[df.Class == 0]\n",
    "\n",
    "frauds.shape\n",
    "normal.shape\n",
    "#Summary Statistics\n",
    "df.describe()\n",
    "x = raw_data.drop('Class', axis=1)\n",
    "y=raw_data['Class']\n",
    ")\n",
    "yra duta 'Class\"]\n",
    "strain val.x test,y train val.y_test-train test split(x,y, test size 0.15, random state, stratify y) train, val,\n",
    "ytrain, y val train test split(x train val, y_train_val, test size 0.15, random state-o, stratify y_train_val)\n",
    "attack data rau data[rau data[class]→1) normal datara data[ra data[ Class]]\n",
    "print(fluster of Source Attack Record print(\"Nunber of Source Normal Record (len(attack data)) with\n",
    "ratio (round((len(attack_data)/len(attack_data)/len (raw_data))*100,4))) (len(normal data)) with ratio\n",
    "(round((len(normal_data)/len(normal_data)/len(raw_data))*100,4)) 5')\n",
    "ass\"], sort = True)\n",
    "printf sumber of test attack Record protuber of test Normal Record (y_test.value_counts()[1]) with\n",
    "ratio (round(y_test.value_counts()[1]/len(y_test)*180,4)) X') (y_test.value_counts()[0]) with ratic\n",
    "(round(y_test.value_counts()[@]/len(y_test)*100,4)) X')\n",
    "San\")\n",
    "Print(arenumber of train Attack Record print Flumber of train Normal Record (y\n",
    "train.value_counts()[1]) with ratio (round(y_train.value_counts()[1]/len(y_train)*100,4)) %\") (y\n",
    "train.value_counts()[0]) with ratio (round(y_train.value counts()[8]/len(y_train)*100,4)) X')\n",
    "print(\"Number of Validate Attack Record protuber of validate Normal Record\n",
    "(y_val.value_counts() [1]) with ratio (round(y_val.value_counts() [1]/len(y_val)*100,4)) X')\n",
    "(y_val.value counts()[0]) with ratio (round(y_val.value_counts()[8]/len(y_val)*100,4)) X')\n",
    "train data-pd.concat(x_train,y train), axis -1)\n",
    "trale data\n",
    "\n",
    "from skleurm.neighbors Inport KleighborsClassifier\n",
    "CM model KheighborsClassifier(n_neighbors. 5) CM model fits train,y train)\n",
    "pred-model.predictix_test from sklearn.linear_model\n",
    "import LogisticRegression\n",
    "model -LogisticRegression(max_iter = 1000)\n",
    "model.fit(x_train,y_train)\n",
    "y_pred_proba KNN_model.predict_proba(x_test)\n",
    "y_pred_proba\n",
    "y_pred_edited-y_pred_proba[:,1]>0.2\n",
    "y_pred_edited-y_pred_edited.astype(int)\n",
    "y pred edited\n",
    "from sklearn import metrics\n",
    "class_report metrics.classification_report(y_test,y_pred)\n",
    "print(class_report)\n",
    "accuaracy metrics.accuracy_score(y_test, y_pred)\n",
    "recall metrics.recall_score(y_test, y_pred)\n",
    "precision metrics.precision_score(y_test,y_pred) f1_score metrics.f1_score(y_test,y_pred)\n",
    "print('Accuaracy, accuaracy.round(3)*100,'%')\n",
    "print('Recall recall.round(3)*100, '%')\n",
    "print('Precision, precision.round(3)*100, '%')\n",
    "print('F1_score, f1_score.round(3)*100, '%')\n",
    "import matplotlib.pyplot as plt\n",
    "fpr, tpr, tresholds metrics.roc_curve(y_test,y_pred_proba[:,1])\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0,1], [0,1], '--')\n",
    "AUC metrics.roc_auc_score(y_test,y_pred_proba[:,1])\n",
    "\n",
    "print AUC AUC.round(3) 100%\n",
    "✓ Normalization\n",
    "from sklearn.preprocessing inport No\n",
    "normalizer Normalizer (norm-'12\")\n",
    "print(normalizer.fit_transform(df))\n",
    "#distribution of legitimate transac\n",
    "df['Class\").value_counts()\n",
    "#separating the data for analysis\n",
    "legit df(df['Class\"]\n",
    "fraud dfdf['Class\"]\n",
    "11\n",
    "#statistical measures of the legit d\n",
    "legit.describe()\n",
    "legit.Amount.describe() statistical measures of the fraud da\n",
    "# fraud.describe()\n",
    "fraud.Amount.describe()\n",
    "compare the values for both transact df.groupby (Class\").mean\n",
    "legit sample legit.sample(n-492)\n",
    "Concatenating two DataFrames\n",
    "new_df pd.concat([legit_sample, fraud\n",
    "Print first 5 rows of the new dataset\n",
    "new_df.head()\n",
    "Getting the distribution of the class\n",
    "new df['Class\"].value_counts()\n",
    "Check Missing values\n",
    "\n",
    "df.isnull().sum().sort values(ascending\n",
    "new df.shape\n",
    "Splitting the data into Features & Tar new_df.drop(columns-Class, axis-1)\n",
    "New\n",
    "Splitting the data into Training data & Testing data\n",
    "x_train, x_test, y train, y test train_test_split(x, y, test sizeve.2, random state-42)\n",
    "Check whether the data is splitted in 80:20 ratio print(x.shape, X_train.shape, x_test.shape)\n",
    "Check whether the data is splitted in se:20 ratio print(X.shape, x_train.shape, x_test.shape)\n",
    "#call the Hodel\n",
    "model LogisticRegression(random_state=42)\n",
    "import pipeline\n",
    "from sklearn.pipeline import Pipeline Create a pipeline for each model\n",
    "pipeline Pipeline([\n",
    "('model', model)\n",
    "1)\n",
    "Perform cross-validation\n",
    "scores cross val_score(pipeline, x_train, y train, cv-5)\n",
    "Calculate mean accuracy\n",
    "mean accuracy scores.mean()\n",
    "Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y train) Make predictions on the test data\n",
    "y pred pipeline.predict(X_test)\n",
    "calculate accuracy score\n",
    "accuracy accuracy_score(y_test, y_pred)\n",
    "print(\"Hodel\", LogisticRegression())\n",
    "print(\"Cross-validation Accuracy:\", mean accuracy) print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "print('Recall score: recall score(y_test, y_pred)) print('Precision Score:', precision_score(y_test, y\n",
    "pred)>\n",
    "best model pipeline\n",
    "#save the best model\n",
    "import pickle\n",
    "pickle.dump(best_model, open('iris_model.dot', 'wb'))\n",
    "#visulalizing the confusion matrix\n",
    "LABELS [Normal', 'Fraud']\n",
    "from sklearn.metrics import confusion_matrix conf matrix confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize =(6, 6))\n",
    "sns.heatmap(conf_matrix, xticklabels LABELS, yticklabels LABELS, annot True, fat \"d\");\n",
    "plt.title(\"Confusion matrix\") plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.show\n",
    "Input Input (shape-10, nameInput')\n",
    "Encaster Layers encoder Dense (64, activation-relu\", name-encoder) (Input)\n",
    "encoder Dense(33, activations'relu, nas ancoder Densete, activatione relu, name-encoder) (encoder)\n",
    "Decoder Layers\n",
    "decader Dense (17, activation rela, name decoder) (encoder)\n",
    "decoder Densa of, activation relu, name decoder) (decoder) decoder Dunse(10, activation-lo, name\n",
    "decolor) (decoder)\n",
    "Define model\n",
    "modal Model(Inputs-Input, outputs-decoder)\n",
    "nodel.summary\n",
    "mode) compile(optimizera, lasse Input layer Input (shape(X. shape(1).3)\n",
    "\n",
    "encoding part\n",
    "encoded Dense(10), activation tanh, artivity, regularlsregilarters.11(-1)t layer) encodet Dense(se,\n",
    "activation ralu) (encoded)\n",
    "we decaling part\n",
    "decoded Dense (se, activation tanh) (encoder) decoded Dansa 100, activation tanh decoded\n",
    "## output lever\n",
    "output Layer Dense (Kshape(1), activation pels) (decoded) autoencoder Model(Input layer, output\n",
    "layer)\n",
    "data.drop(Class), ace) y data Class\" values\n",
    "autuencoder adulte, louss scalexicaler), fit transforms,values Tors(values) nere, frautscale(y),\n",
    "xscalely1] autoencoder.fit(x_norm[0:2000], x_norm[6:2000], batch size 512, epochs 30,\n",
    "shuffle True, validation split 8.20);\n",
    "hidden_representation Sequential()\n",
    "hidden representation.add(autoencoder.layers[0])\n",
    "hidden representation.add(autoencoder.layers[1])\n",
    "hidden_representation.add(autoencoder.layers[2])\n",
    "norm_hid_rep hidden_representation.predict(x_norm[:3000])\n",
    "fraud_hid_rep hidden_representation.predict(x_fraud) rep x np.append(norm_hid_rep,\n",
    "fraud_hid_rep, axis 0)\n",
    "y_nnp.zeros(norm_hid_rep.shape[0])\n",
    "y_fnp.ones(fraud_hid_rep.shape[0])\n",
    "rep_y np.append(y_n, y_f)\n",
    "train x, val x, train y, valy train_test_split(rep_x, rep_y, test size-0.25)\n",
    "clf LogisticRegression (solver-\"1bfgs\").fit(train x, train y)\n",
    "pred_y clf.predict(val_x)\n",
    "print(\"\")\n",
    "\n",
    "print (\"Classification Report: \")\n",
    "print (classification_report(val_y, pred_y))\n",
    "print (\"\")\n",
    "print Accuracy Score:\", accuracy_score(val_y, pred_y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
